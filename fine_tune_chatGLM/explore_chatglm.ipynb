{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/samsong/anaconda3/envs/pytorchbook/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 115\n",
      "CUDA SETUP: Loading binary /home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda115.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, load_in_8bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么再快的86也追不上坐着夏树的奔驰? 这句话的表面意思是:即使86号车再快,也无法追上一辆坐着夏树(奔驰)的车。\n",
      "\n",
      "这句话背后的含义是:速度与时间的关系。\n",
      "\n",
      "86号车和夏树(奔驰)的车在速度上的差异,可能是因为86号车的速度比夏树(奔驰)的车慢,或者是因为夏树(奔驰)的车比86号车更快。此外,时间也是一个重要的因素。86号车和夏树(奔驰)的车在时间上的差异,可能是因为夏树(奔驰)的车比\n"
     ]
    }
   ],
   "source": [
    "input_text = \"为什么再快的86也追不上坐着夏树的奔驰?\"\n",
    "\n",
    "ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "\n",
    "outs = model.generate(ids, max_length=128)\n",
    "\n",
    "response = tokenizer.batch_decode(outs)\n",
    "print(response[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Lora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results without fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "model.config.use_cache = (\n",
    "    False  # silence the warnings. Please re-enable for inference!\n",
    ")\n",
    "\n",
    "prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "{\n",
    "    \"context\": \"大师，我最近事业不顺利，感觉很迷茫，该怎么办？\",\n",
    "    \"target\": \"孩子，生活中的起伏和挫折是常态。放下烦恼，用平静的心态面对困境，努力奋斗，同时保持内心的善良与慈悲，你将找到属于你的光明之路。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"师父，我与家人之间常常发生争吵，如何改善家庭关系？\",\n",
    "    \"target\": \"阿弥陀佛，亲情是一份宝贵的财富，要从宽容和理解开始。培养平和的心态，倾听彼此的意见，以慈悲与智慧去化解冲突，共同创造和谐温暖的家庭氛围。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"师傅，我经常感到焦虑和压力，怎样才能获得内心的宁静？\",\n",
    "    \"target\": \"孩子，内心的宁静源于觉察和自我调适。修行冥想，深呼吸，寻找内心的栖息地。明白外在的变幻无常，放下过多的执着与欲望，你将发现内心的平静与安宁。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"大师，我常常陷入烦恼和抱怨，怎样才能培养感恩之心？\",\n",
    "    \"target\": \"施主，感恩是一种修养，需要从日常小事开始。每天意识到自己的幸运和获得，体察他人的善意和关爱。珍惜当下，感恩生活中的点滴，你将发现更多的快乐与满足。\",\n",
    "},\n",
    "{\n",
    "    \"context\": \"师父，我对未来感到迷茫和恐惧，不知道自己的人生目标是什么，有何建议？\",\n",
    "    \"target\": \"阿弥陀佛，人生的方向不一定要一目了然，它是通过不断尝试和探索而逐渐清晰的。保持对未知的勇敢与开放，寻找自己内心的声音。用心去体验，用智慧去选择，你将找到属于自己的道路。\",\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问:大师,我最近事业不顺利,感觉很迷茫,该怎么办?\n",
      "答: \n",
      "\n",
      "首先,我很抱歉听到您的事业不顺利。但是,请不要放弃,因为事业迷茫是一种常见的情绪,很多人都会经历。以下是一些建议,希望对您有所帮助:\n",
      "\n",
      "1. 找出问题所在:明确事业不顺利的原因。是因为缺乏灵感、缺乏创意、缺乏动力,还是其他原因?一旦确定了问题,就可以开始寻找解决方案。\n",
      "\n",
      "2. 寻求帮助:如果感到迷茫,可以寻求专业人士的帮助,例如咨询师、心理医生或职业顾问。他们可以帮助理清思路,提供建议和指导。\n",
      "\n",
      "3.\n",
      "问:师父,我与家人之间常常发生争吵,如何改善家庭关系?\n",
      "答: 家庭关系是人生中非常重要的一部分,而争吵是家庭中常见的问题。以下是一些可能有助于改善家庭关系的建议:\n",
      "\n",
      "1. 沟通:沟通是解决冲突的关键。尝试与家人坦诚地交流,听取他们的观点,并尝试找到共同点。在交流过程中,尽量避免指责和攻击,而是尝试解决问题。\n",
      "\n",
      "2. 尊重:尊重是建立和谐关系的基础。尊重家人的感受和观点,并尝试理解他们的立场。即使不同意他们的观点,也要尊重他们的意见,避免争吵。\n",
      "\n",
      "3.\n",
      "问:师傅,我经常感到焦虑和压力,怎样才能获得内心的宁静?\n",
      "答: 焦虑和压力是常见的情绪体验,但可以通过一些方法来缓解和克服它们。以下是一些建议:\n",
      "\n",
      "1. 深呼吸:深呼吸可以帮助你放松身体和心灵。尝试缓慢地吸气,然后慢慢地呼气,重复几次。\n",
      "\n",
      "2. 冥想:冥想是一种放松和集中注意力的方法。找一个安静的地方,坐下来,闭上眼睛,专注于呼吸或一个特定的冥想练习。\n",
      "\n",
      "3. 运动:运动可以帮助你释放紧张情绪和压力,并提高身体的代谢率。选择一种你喜欢的运动方式,\n",
      "问:大师,我常常陷入烦恼和抱怨,怎样才能培养感恩之心?\n",
      "答: 感恩之心是一种积极的情感,可以帮助我们更好地珍惜身边的人和事,并在生活中获得更多的快乐和满足感。以下是一些培养感恩之心的建议:\n",
      "\n",
      "1. 意识到自己的感恩之心:要意识到感恩是一种积极的情感,而不是一种消极的情绪。当意识到自己正在感恩时,就可以开始思考身边的美好事物,并将它们纳入自己的行动计划中。\n",
      "\n",
      "2. 感恩身边的人:感恩不仅仅是针对家人和亲人,还可以包括朋友、同事、邻居、社区和其他人。要时刻注意身边的美好事物,并表达感激之情。\n",
      "\n",
      "3\n",
      "问:师父,我对未来感到迷茫和恐惧,不知道自己的人生目标是什么,有何建议?\n",
      "答: 对未来感到迷茫和恐惧是非常常见的感受,很多人都会经历这种情况。以下是一些建议,希望能帮助找到人生目标:\n",
      "\n",
      "1. 探索自己的兴趣和热情:尝试回想一下过去自己最喜欢做的事情,或者最近感到最兴奋的事情。这些可能是自己的兴趣和热情所在,也可能是未来想要追求的方向。\n",
      "\n",
      "2. 设定短期和长期目标:将目标分解成短期和长期目标,更容易实现和评估进展。短期目标可以是每天或每周的任务,长期目标可以是一年或更长时间的计划\n"
     ]
    }
   ],
   "source": [
    "for item in datas:\n",
    "    text = f\"问:{item['context']}\\n答:\"\n",
    "    ids = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "    outs = model.generate(input_ids=ids, max_length=128)\n",
    "    print(tokenizer.batch_decode(outs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset_rows import preprocess\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = [preprocess(tokenizer, model.config, item, max_seq_length=256) for item in datas]\n",
    "\n",
    "dataset = Dataset.from_list(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▎        | 10/80 [00:17<02:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0652, 'learning_rate': 9e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20/80 [00:34<01:43,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5896, 'learning_rate': 7.75e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 30/80 [00:52<01:24,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3095, 'learning_rate': 6.625e-05, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40/80 [01:08<01:06,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.488, 'learning_rate': 5.375e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 50/80 [01:26<00:52,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.12, 'learning_rate': 4.125e-05, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [01:43<00:35,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0491, 'learning_rate': 2.8749999999999997e-05, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 70/80 [02:01<00:17,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0311, 'learning_rate': 1.6250000000000002e-05, 'epoch': 70.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:19<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0237, 'learning_rate': 3.75e-06, 'epoch': 80.0}\n",
      "{'train_runtime': 139.0364, 'train_samples_per_second': 2.877, 'train_steps_per_second': 0.575, 'train_loss': 1.0845098892226814, 'epoch': 80.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=1.0845098892226814, metrics={'train_runtime': 139.0364, 'train_samples_per_second': 2.877, 'train_steps_per_second': 0.575, 'train_loss': 1.0845098892226814, 'epoch': 80.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetune import ModifiedTrainer, data_collator\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    fp16 =True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 5,\n",
    "    learning_rate = 1e-4,\n",
    "    num_train_epochs=80,\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results with LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/samsong/anaconda3/envs/pytorchbook/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问:大师,我最近事业不顺利,感觉很迷茫,该怎么办?\n",
      "答: 孩子,生活中的起伏和挫折是常态。放下烦恼,用平静的心态面对困境,努力奋斗,同时保持内心的善良与慈悲,你将找到属于你的光明之路。\n",
      "问:师父,我与家人之间常常发生争吵,如何改善家庭关系?\n",
      "答: 阿弥陀佛,亲情是一份宝贵的财富,要从宽容和理解开始。培养平和的心态,倾听彼此的意见,以慈悲与智慧去化解冲突,共同创造和谐温暖的家庭氛围。\n",
      "问:师傅,我经常感到焦虑和压力,怎样才能获得内心的宁静?\n",
      "答: 孩子,内心的宁静源于觉察和自我调适。修行冥想,深呼吸,寻找内心的栖息地。明白外在的变幻无常,放下过多的执着与欲望,你将发现内心的平静与安宁。\n",
      "问:大师,我常常陷入烦恼和抱怨,怎样才能培养感恩之心?\n",
      "答: 施主,感恩是一种修养,需要从日常小事开始。每天意识到自己的幸运和获得,体察他人的善意和关爱。珍惜当下,感恩生活中的点滴,你将发现更多的快乐与满足。\n",
      "问:师父,我对未来感到迷茫和恐惧,不知道自己的人生目标是什么,有何建议?\n",
      "答: 阿弥陀佛,人生的方向不一定要一目了然,它是通过不断尝试和探索而逐渐清晰的。保持对未知的勇敢与开放,寻找自己内心的声音。用心去体验,用智慧去选择,你将找到属于自己的道路。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.config.use_cache = (\n",
    "    True\n",
    ")\n",
    "\n",
    "for item in datas:\n",
    "    text = f\"问:{item['context']}\\n答:\"\n",
    "    ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    outs = model.generate(input_ids=ids, max_length=128)\n",
    "    print(tokenizer.batch_decode(outs)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.peft_model import PeftModelForCausalLM\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"\"\"原句子：一个优雅的粉白色百合在宁静的花园中绽放。柔软的花瓣轻轻带有粉色，而茎上浓绿的叶子形成了生机盎然的对比。\n",
    "人：帮我将百合花改成玫瑰花\n",
    "修改后：一个优雅的粉白色百合在宁静的花园中绽放。柔软的花瓣轻轻带有粉色，而茎上浓绿的叶子形成了生机盎然的对比。\"\"\",\n",
    "\n",
    "\"\"\"原句子：一朵充满生机的仙人掌花盛开着，它的鲜艳粉红色花瓣和明亮的黄色芯与多刺的绿色仙人掌植物形成了对比。\n",
    "人：请把花瓣涂成紫色\n",
    "修改后：一朵充满生机的仙人掌花盛开着，它的鲜艳粉红色花瓣和明亮的黄色芯与多刺的绿色仙人掌植物形成了对比。\"\"\",\n",
    "\n",
    "\"\"\"原句子：一个女孩头戴着一件白色宽松袖口的衬衫和一条长裙。她手持画笔，站在一个充满多彩花朵的花园中。背景是蓝天和蓬松的云朵。\n",
    "人：请将多彩花朵替换为充满活力的花朵。\n",
    "修改后：一个女孩头戴着一件白色宽松袖口的衬衫和一条长裙。她手持画笔，站在一个充满充满活力的花朵的花园中。背景是蓝天和蓬松的云朵。\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原句子:两个演员在一场京剧表演中进行激动人心的剑术对决。他们的动作流畅而精确,以他们对这门艺术的掌握能够吸引观众。\n",
      "人:请删除剑术对决的场景。\n",
      "\n",
      "例如:\n",
      "原句子:一个优雅的粉白色百合在宁静的花园中绽放。柔软的花瓣轻轻带有粉色,而茎上浓绿的叶子形成了生机盎然的对比。\n",
      "人:帮我将百合花改成玫瑰花\n",
      "修改后:一个优雅的粉白色百合在宁静的花园中绽放。柔软的花瓣轻轻带有粉色,而茎上浓绿的叶子形成了生机盎然的对比。 原句子:一朵充满生机的仙人掌花盛开着,它的鲜艳粉红色花瓣和明亮的黄色芯与多刺的绿色仙人掌植物形成了对比。\n",
      "人:请把花瓣涂成紫色\n",
      "修改后:一朵充满生机的仙人掌花盛开着,它的鲜艳粉红色花瓣和明亮的黄色芯与多刺的绿色仙人掌植物形成了对比。 原句子:一个女孩头戴着一件白色宽松袖口的衬衫和一条长裙。她手持画笔,站在一个充满多彩花朵的花园中。背景是蓝天和蓬松的云朵。\n",
      "人:请将多彩花朵替换为充满活力的花朵。\n",
      "修改后:一个女孩头戴着一件白色宽松袖口的衬衫和一条长裙。她手持画笔,站在一个充满充满活力的花朵的花园中。背景是蓝天和蓬松的云朵。 原句子:两个演员在一场京剧表演中进行激动人心的剑术对决。他们的动作流畅而精确,以他们对这门艺术的掌握能够吸引观众。\n",
      "人:删除剑术对决的场景。\n",
      "修改后:两个演员在一场京剧表演中展现他们流畅而精确的动作,以他们对这门艺术的掌握能够吸引观众。\n",
      " 两个演员在一场京剧表演中展现他们流畅而精确的动作,以他们对这门艺术的掌握能够吸引观众。\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"\n",
    "原句子：两个演员在一场京剧表演中进行激动人心的剑术对决。他们的动作流畅而精确，以他们对这门艺术的掌握能够吸引观众。\n",
    "人：请删除剑术对决的场景。\n",
    "\n",
    "例如:\n",
    "{' '.join(examples)}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "outs = model.base_model.generate(ids, max_length=512)\n",
    "out_text = tokenizer.decode(outs[0])\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
